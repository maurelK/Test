#!/usr/bin/env python3

import numpy as np
import sys
import os
sys.path.append(os.path.dirname(__file__))

from my_torch.Neuron import Neuron

# Mapping des labels vers indices
LABEL_TO_INDEX = {
    'Nothing': 0,
    'Check': 1,
    'Checkmate': 2
}

INDEX_TO_LABEL = {v: k for k, v in LABEL_TO_INDEX.items()}

def load_chess_dataset(filename):
    """Charge le dataset d'√©checs depuis un fichier"""
    X = []
    y = []

    with open(filename, 'r') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue

            parts = line.split()
            if len(parts) < 7:  # FEN complet + au moins 1 label
                print(f"Ligne {line_num} ignor√©e: pas assez de parties ({len(parts)})")
                continue

            # Gestion sp√©ciale pour les fichiers nothing qui n'ont qu'un label
            if len(parts) == 7:  # FEN + Nothing
                fen = ' '.join(parts[:-1])
                label_full = parts[-1]
            else:  # FEN + label + couleur
                fen = ' '.join(parts[:-2])
                label_full = ' '.join(parts[-2:])

            # Extraire le type de label (Check, Checkmate, Nothing)
            if 'Checkmate' in label_full:
                label = 'Checkmate'
            elif 'Check' in label_full:
                label = 'Check'
            else:
                label = 'Nothing'

            # Convertir FEN en vecteur one-hot
            vector = fen_to_vector(fen)
            X.append(vector.flatten())

            # Convertir label en one-hot
            label_idx = LABEL_TO_INDEX[label]
            one_hot = np.zeros(3)
            one_hot[label_idx] = 1
            y.append(one_hot)

    X = np.array(X).T  # Shape: (768, n_samples)
    y = np.array(y).T  # Shape: (3, n_samples)

    print(f"Dataset charg√©: {X.shape[1]} √©chantillons")
    print(f"Distribution des classes:")
    for label, idx in LABEL_TO_INDEX.items():
        count = np.sum(y[idx, :])
        print(f"  {label}: {int(count)} √©chantillons")

    return X, y

def fen_to_vector(fen):
    """Convertit une cha√Æne FEN en vecteur one-hot 768 √©l√©ments (12 canaux √ó 64 cases)"""
    board_part = fen.split()[0]  # Prendre seulement la partie plateau
    
    # Mapping des pi√®ces vers les canaux (0-5: blanc, 6-11: noir)
    piece_map = {
        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,      # Pi√®ces blanches
        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11     # Pi√®ces noires
    }
    
    vector = np.zeros(768)  # 12 canaux √ó 64 cases
    square_idx = 0
    
    for char in board_part:
        if char.isdigit():
            # Sauter les cases vides
            square_idx += int(char)
        elif char == '/':
            # Sauter les s√©parateurs de rang√©es
            continue
        elif char in piece_map:
            # Activer le canal correspondant pour cette pi√®ce
            channel = piece_map[char]
            vector[channel * 64 + square_idx] = 1
            square_idx += 1
    
    return vector.reshape(-1, 1)

def create_chess_network():
    """Cr√©e un r√©seau de neurones optimis√© pour l'analyse d'√©checs"""
    print("Cr√©ation du r√©seau d'analyse d'√©checs optimis√©...")

    network = Neuron(
        loss='categorical_crossentropy',
        learning_rate=0.001,  # Learning rate r√©duit pour stabilit√©
        l2_lambda=0.0001      # R√©gularisation L2 l√©g√®re
    )

    # Architecture optimis√©e avec plus de neurones et dropout progressif
    network.add_layer(768, 512, 'relu', dropout_rate=0.3)    # Entr√©e one-hot -> Couche cach√©e 1
    network.add_layer(512, 256, 'relu', dropout_rate=0.25)   # Couche cach√©e 1 -> Couche cach√©e 2
    network.add_layer(256, 128, 'relu', dropout_rate=0.2)    # Couche cach√©e 2 -> Couche cach√©e 3
    network.add_layer(128, 64, 'relu', dropout_rate=0.1)     # Couche cach√©e 3 -> Couche cach√©e 4
    network.add_layer(64, 3, 'softmax', dropout_rate=0.0)    # Couche cach√©e 4 -> Sortie

    print("Architecture optimis√©e du r√©seau:")
    print("  Entr√©e: 768 neurones (encodage one-hot FEN)")
    print("  Couche cach√©e 1: 512 neurones (ReLU + Dropout 30%)")
    print("  Couche cach√©e 2: 256 neurones (ReLU + Dropout 25%)")
    print("  Couche cach√©e 3: 128 neurones (ReLU + Dropout 20%)")
    print("  Couche cach√©e 4: 64 neurones (ReLU + Dropout 10%)")
    print("  Sortie: 3 neurones (Softmax)")
    print("  R√©gularisation: L2 Œª=0.0001, Learning rate=0.001")

    return network

def evaluate_network(network, X, y):
    """√âvalue les performances du r√©seau"""
    predictions = network.predict(X)
    pred_classes = np.argmax(predictions, axis=0)
    true_classes = np.argmax(y, axis=0)

    accuracy = np.mean(pred_classes == true_classes)
    print(".2f")

    # Matrice de confusion simple
    print("Matrice de confusion:")
    for true_idx in range(3):
        for pred_idx in range(3):
            count = np.sum((true_classes == true_idx) & (pred_classes == pred_idx))
            true_label = INDEX_TO_LABEL[true_idx]
            pred_label = INDEX_TO_LABEL[pred_idx]
            print(f"  {true_label} -> {pred_label}: {int(count)}")

    return accuracy

def train_chess_network_enhanced(dataset_file, epochs=200, batch_size=64, patience=10, 
                               save_path='my_torch_network_enhanced.nn'):
    """Entra√Æne le r√©seau avec am√©liorations : early stopping, validation, learning rate decay"""
    print("=== Entra√Ænement am√©lior√© du r√©seau d'analyse d'√©checs ===")

    # Charger les donn√©es
    X, y = load_chess_dataset(dataset_file)
    
    # Split train/validation (80/20)
    n_samples = X.shape[1]
    n_train = int(0.8 * n_samples)
    
    indices = np.random.permutation(n_samples)
    train_indices = indices[:n_train]
    val_indices = indices[n_train:]
    
    X_train = X[:, train_indices]
    y_train = y[:, train_indices]
    X_val = X[:, val_indices]
    y_val = y[:, val_indices]
    
    print(f"Split: {n_train} train, {n_samples - n_train} validation samples")

    # Cr√©er le r√©seau
    network = create_chess_network()

    print(f"\nD√©but de l'entra√Ænement sur {X_train.shape[1]} √©chantillons...")
    print(f"Nombre d'√©poques max: {epochs}, Patience: {patience}, Batch size: {batch_size}")

    # Variables pour early stopping
    best_val_accuracy = 0
    patience_counter = 0
    best_weights = None
    
    # Learning rate decay
    initial_lr = network.learning_rate
    decay_rate = 0.95
    decay_steps = 20

    for epoch in range(epochs):
        # Learning rate decay
        if epoch > 0 and epoch % decay_steps == 0:
            network.learning_rate *= decay_rate
            print(".6f")

        # Entra√Ænement
        history = network.train(X_train, y_train, X_val, y_val, epochs=1, batch_size=batch_size, verbose=False)
        
        train_loss = history['train_loss'][0]
        train_acc = history['train_accuracy'][0]
        val_loss = history['val_loss'][0] 
        val_acc = history['val_accuracy'][0]
        
        print(f"√âpoque {epoch + 1:2d} - Train: Loss={train_loss:.4f}, Acc={train_acc:.1f}% | Val: Loss={val_loss:.4f}, Acc={val_acc:.1f}%")
        # Early stopping
        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            patience_counter = 0
            best_weights = [layer.weight.copy() for layer in network.layers]
            print(f"  ‚Üí Nouveau meilleur mod√®le (validation: {best_val_accuracy:.1f}%)")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"\nüõë Early stopping apr√®s {epoch + 1} √©poques (patience √©puis√©e)")
            break
    
    # Restaurer les meilleurs poids
    if best_weights:
        for i, layer in enumerate(network.layers):
            layer.weight = best_weights[i]
        print("‚úÖ Meilleurs poids restaur√©s")
    
    print("\n√âvaluation finale sur validation...")
    final_val_loss, final_val_accuracy = network.evaluate(X_val, y_val)
    print(f"Pr√©cision finale sur validation: {final_val_accuracy:.2f}%")
    # Sauvegarder le mod√®le entra√Æn√©
    network.save(save_path)
    print(f"\nüíæ Mod√®le sauvegard√©: {save_path}")

    return network, final_val_accuracy

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Entra√Æner un r√©seau MY_TORCH sur des donn√©es d\'√©checs')
    parser.add_argument('dataset', help='Fichier contenant les donn√©es d\'entra√Ænement (FEN + labels)')
    parser.add_argument('--epochs', type=int, default=200, help='Nombre d\'√©poques d\'entra√Ænement maximum')
    parser.add_argument('--batch-size', type=int, default=64, help='Taille des mini-batches')
    parser.add_argument('--patience', type=int, default=10, help='Patience pour early stopping')
    parser.add_argument('--save', default='my_torch_network_enhanced.nn', help='Chemin de sauvegarde du mod√®le')

    args = parser.parse_args()

    try:
        network, accuracy = train_chess_network_enhanced(
            args.dataset, 
            epochs=args.epochs,
            batch_size=args.batch_size,
            patience=args.patience,
            save_path=args.save
        )
        print(f"\nüéØ Pr√©cision finale sur validation: {accuracy:.2f}")
    except Exception as e:
        print(f"Erreur lors de l'entra√Ænement: {e}", file=sys.stderr)
        sys.exit(84)